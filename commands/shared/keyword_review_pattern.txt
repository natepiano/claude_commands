# Shared Keyword Review Pattern Instructions
# This file contains common patterns for interactive keyword-driven review processes
# Reference specific tagged sections from review command files

<UnifiedInvestigationInstructions>
## Investigation Agent Instructions Template

This template provides standardized investigation agent instructions for all review types.

### Investigation Agent Instruction Pattern

**CRITICAL**: Pass the COMPLETE [ITEM] to each investigation agent, including the Current and Proposed code examples

**CRITICAL**: Instruct each investigation agent with these EXACT words:
"‚ö†Ô∏è CRITICAL FAILURE CONDITION: You MUST include Current State and Proposed Change sections WITH CODE BLOCKS following the EXACT template from <InvestigationAgentInstructions>.
IF YOU CANNOT PROVIDE CODE BLOCKS, respond with 'INVALID: Cannot provide required code examples [reason]' - DO NOT provide a partial response.
Any response missing code blocks will be REJECTED and discarded."

**Investigation specifics**:
- **DOMAIN VALUES**: [DOMAIN_VALUES]
- **INVESTIGATION FOCUS**: [INVESTIGATION_FOCUS_LIST]
- **EXPECTED VERDICTS**: [EXPECTED_VERDICTS]

### Investigation Workflow Steps

1. Filter out any [ITEMS] already [RESOLUTION_STATE]
2. Launch parallel investigation agents for ALL remaining [ITEMS] using multiple Task tool invocations in a single response
3. Apply the standardized investigation instructions above with review-specific parameters
4. Wait for all investigations to complete
5. **VALIDATION**: Check each investigation result - REJECT any that don't use the template with code blocks
6. Merge compliant [ITEMS] with their investigation results

### Parameter Definitions

When using this investigation template, each review command must provide:

- **[ITEM]**: Singular name for review item (e.g., "recommendation", "issue", "discrepancy")
- **[ITEMS]**: Plural name for review items (e.g., "recommendations", "issues", "discrepancies")
- **[RESOLUTION_STATE]**: How items get filtered (e.g., "already in Skip Notes", "already fixed", "already resolved")
- **[DOMAIN_VALUES]**: Investigation domain focus (e.g., "code elegance AND practical utility")
- **[INVESTIGATION_FOCUS_LIST]**: Bullet points of specific investigation criteria
- **[EXPECTED_VERDICTS]**: Possible investigation outcomes (e.g., "CONFIRMED, MODIFIED, or REJECTED")

### Investigation Focus Examples

**Design Review Focus**:
- Evaluate architectural merit and system design improvements
- Assess type system usage to eliminate bugs and express intent
- Consider API elegance and interface design quality
- Analyze pattern consistency with existing codebase
- Check for over-engineering vs solving real problems
- Consider how this affects future evolution and extensions

**Code Review Focus**:
- Assess actual bug risk vs stylistic preference
- Evaluate fix complexity and refactoring scope
- Consider impact on code clarity and readability
- Check for performance implications
- Analyze maintenance burden reduction
- Verify fix won't introduce new issues
- Consider alignment with team conventions

**Plan Alignment Focus**:
- Understand why the implementation diverged from plan
- Evaluate technical merit of both approaches
- Check if implementation discovered improvements
- Assess complexity/risk of alignment effort
- Consider unforeseen technical constraints
- Analyze business/functional impact
- Determine if plan should be updated instead
</UnifiedInvestigationInstructions>

<StandardOutputFormat>
## Universal Review Output Format Template

This template provides the standard output format structure used by all review types when subagents return findings.

### Review Summary Template

```
### [REVIEW_SUMMARY_TITLE]
Brief overview of [REVIEW_TARGET_DESCRIPTION] (1-3 sentences max)

**Metrics:**
- ‚úÖ [POSITIVE_METRIC_LABEL]: [COUNT]
- üîß [ACTION_METRIC_LABEL]: [COUNT]

**[POSITIVE_FINDINGS_SECTION_TITLE]:**
- [Brief bullet point about each positive aspect found]
- [e.g., domain-specific positive examples]
- [e.g., good practices observed in the review target]

### [FINDINGS_SECTION_TITLE]
**IMPORTANT: Only include actual [PROBLEMS_DESCRIPTION] that need [ACTION_TYPE]. Do NOT include positive examples as [FINDINGS_TYPE].**
```

### Category-Based Finding Template

```
‚ïê‚ïê‚ïê [CATEGORY-TYPE]-[ID]: [Brief title] ‚ïê‚ïê‚ïê
Priority: [High/Medium/Low] | Category: [CATEGORY-TYPE]

[PRIMARY_FIELD_LABEL]: [Primary field description]
[LOCATION_FIELD]: [File path and line numbers or location info]

[CURRENT_STATE_FIELD]:
    [Current code/content snippet - indented for clarity]

[PROPOSED_CHANGE_FIELD]:
    [Improved/suggested version - indented for clarity]

[IMPACT_FIELD_LABEL]: [Impact/rationale/benefit description]
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
```

### Parameter Definitions

When using this output format template, each review command must provide:

**Summary Parameters:**
- **[REVIEW_SUMMARY_TITLE]**: Summary section name (e.g., "Review Summary", "Alignment Summary")
- **[REVIEW_TARGET_DESCRIPTION]**: What was reviewed (e.g., "changes reviewed", "alignment status")
- **[POSITIVE_METRIC_LABEL]**: Positive findings metric (e.g., "Positive findings", "Fully aligned items")
- **[ACTION_METRIC_LABEL]**: Action items metric (e.g., "Suggested fixes", "Discrepancies found")
- **[POSITIVE_FINDINGS_SECTION_TITLE]**: Positive section title (e.g., "Positive Findings", "Well-Aligned Areas")

**Findings Parameters:**
- **[FINDINGS_SECTION_TITLE]**: Main findings section (e.g., "Actionable Issues", "Alignment Discrepancies")
- **[PROBLEMS_DESCRIPTION]**: Type of problems (e.g., "problems", "discrepancies", "gaps")
- **[ACTION_TYPE]**: Required action (e.g., "fixing", "alignment", "improvement")
- **[FINDINGS_TYPE]**: Findings item type (e.g., "issues", "discrepancies", "recommendations")

**Category Parameters:**
- **[CATEGORY-TYPE]**: Review-specific categories (e.g., "TYPE-SYSTEM", "MISSING", "QUALITY")
- **[PRIMARY_FIELD_LABEL]**: Main field name (e.g., "Issue", "Plan Gap", "Type System Gap")
- **[SECONDARY_FIELD_LABEL]**: Secondary field (e.g., "Location", "Plan Section", "Current Code Pattern")
- **[LOCATION_FIELD]**: Location field name (e.g., "Location", "Files Expected", "Plan Section")
- **[CURRENT_STATE_FIELD]**: Current state field (e.g., "Current Code", "Current State", "Plan's Current Approach")
- **[PROPOSED_CHANGE_FIELD]**: Proposed change field (e.g., "Suggested Code", "Proposed Change", "Proposed Plan Enhancement")
- **[IMPACT_FIELD_LABEL]**: Impact field (e.g., "Impact", "Consequences", "Benefits")

### Review-Specific Category Examples

**Design Review Categories:**
- TYPE-SYSTEM-*, DESIGN-*, IMPLEMENTATION-*, SIMPLIFICATION-*
- Primary fields: "Plan Issue", "Plan Gap", "Type System Gap", "Over-Engineering in Plan"
- Focus: Plan document improvements and design quality

**Code Review Categories:**
- TYPE-SYSTEM-*, QUALITY-*, COMPLEXITY-*, DUPLICATION-*, SAFETY-*
- Primary fields: "Issue", "Type System Violation", "Unnecessary Complexity", "Duplicated Code", "Safety Concern"
- Focus: Code quality and immediate fixes

**Plan Alignment Categories:**
- MISSING-*, MISMATCH-*, PARTIAL-*, UNPLANNED-*, SPECIFICATION-*
- Primary fields: "Plan Section", "Planned Approach", "Planned Scope", "Addition Description", "Specified Requirement"
- Focus: Implementation vs plan specification alignment

### Format Requirements

**Common Requirements Across All Reviews:**
1. Use markdown code blocks with proper syntax highlighting (```rust, ```markdown)
2. Include specific file paths and line numbers where applicable
3. Provide concrete examples in Current and Proposed fields
4. Ensure all findings are actionable with clear improvement paths
5. Prioritize TYPE-SYSTEM issues as highest priority across all review types
6. Include brief but descriptive titles that capture the essence of each finding
</StandardOutputFormat>

<FinalSummaryTemplate>
## Standardized Final Summary Template

This template provides the completion summary format used by all review types when wrapping up interactive review sessions.

### Final Summary Structure

```
[REVIEW_TYPE] Complete!

**Results Summary:**
- [PRIMARY_ACTION_ITEMS]: [X]
[PRIMARY_CATEGORY_BREAKDOWN]

- [SECONDARY_ACTION_ITEMS]: [Y]
[SECONDARY_ITEMS_LIST]

- [MAINTAINED_ITEMS]: [Z]
[MAINTAINED_ITEMS_DESCRIPTION]

[COMPLETION_STATEMENT]
```

### Parameter Definitions

When using this final summary template, each review command must provide:

- **[REVIEW_TYPE]**: Review name (e.g., "Design Review", "Code Review", "Plan Alignment Review")
- **[PRIMARY_ACTION_ITEMS]**: Main action taken (e.g., "Recommendations Agreed", "Issues Fixed", "Discrepancies Aligned")
- **[PRIMARY_CATEGORY_BREAKDOWN]**: Category-specific breakdown of primary actions
- **[SECONDARY_ACTION_ITEMS]**: Secondary action taken (e.g., "Recommendations Skipped", "Issues Skipped", "Discrepancies Skipped")
- **[SECONDARY_ITEMS_LIST]**: List of secondary items with brief descriptions
- **[MAINTAINED_ITEMS]**: Positive items preserved (e.g., "Deviations Accepted", "Positive Practices Maintained", "Well-Aligned Areas Maintained")
- **[MAINTAINED_ITEMS_DESCRIPTION]**: Description of maintained positive aspects
- **[COMPLETION_STATEMENT]**: Final completion message specific to review type

### Review-Specific Summary Examples

**Design Review Summary:**
- **[PRIMARY_ACTION_ITEMS]**: "Recommendations Agreed"
- **[PRIMARY_CATEGORY_BREAKDOWN]**:
  ```
  - TYPE-SYSTEM: [count]
  - DESIGN: [count]
  - IMPLEMENTATION: [count]
  - SIMPLIFICATION: [count]
  ```
- **[SECONDARY_ACTION_ITEMS]**: "Recommendations Skipped"
- **[MAINTAINED_ITEMS]**: "Deviations Accepted"
- **[COMPLETION_STATEMENT]**: "Plan document updated with notes"

**Code Review Summary:**
- **[PRIMARY_ACTION_ITEMS]**: "Issues Fixed"
- **[PRIMARY_CATEGORY_BREAKDOWN]**:
  ```
  - TYPE-SYSTEM: [count]
  - QUALITY: [count]
  - COMPLEXITY: [count]
  - SAFETY: [count]
  - DUPLICATION: [count]
  ```
- **[SECONDARY_ACTION_ITEMS]**: "Issues Skipped"
- **[MAINTAINED_ITEMS]**: "Positive Practices Maintained"
- **[COMPLETION_STATEMENT]**: "All changes have been applied to your working directory."

**Plan Alignment Summary:**
- **[PRIMARY_ACTION_ITEMS]**: "Discrepancies Aligned"
- **[PRIMARY_CATEGORY_BREAKDOWN]**:
  ```
  - MISSING: [count] items now implemented
  - MISMATCH: [count] items corrected
  - SPECIFICATION: [count] items fixed
  ```
- **[SECONDARY_ACTION_ITEMS]**: "Discrepancies Skipped"
- **[MAINTAINED_ITEMS]**: "Well-Aligned Areas Maintained"
- **[COMPLETION_STATEMENT]**: "Plan and implementation are now synchronized."

### Usage Instructions

Each review command should reference this template at the end of their keyword response sections and provide their specific parameters for customization.
</FinalSummaryTemplate>

<KeywordResponseWorkflow>
## Standardized Keyword Response Workflow

This template provides the common workflow structure used by all review types when processing user keyword responses.

### Standard Workflow Pattern

All keyword responses follow this identical structure:
1. **EXECUTE SPECIFIC ACTION**: [REVIEW_SPECIFIC_ACTION]
2. **MARK COMPLETION**: Mark current todo as completed
3. **CRITICAL CUMULATIVE REVIEW**: Before presenting the next [ITEM], review ALL changes made in this session so far. [CUMULATIVE_REVIEW_SPECIFICS]
4. **PRESENT NEXT ITEM**: Present next [ITEM] with its investigation verdict, then use the format from `<KeywordPresentationTemplate>` to show available keywords
5. **STOP AND WAIT**: STOP and wait for user response

### "investigate further" Response (Universal)

**When user responds "investigate further"**:
1. **ASK FOR GUIDANCE**: "What specific aspect would you like me to investigate further?"
2. **WAIT FOR USER INPUT**: Get their specific investigation focus
3. **TASK FOCUSED INVESTIGATION**: Launch new investigation with user's guidance
4. **PRESENT SUPPLEMENTAL FINDINGS**: Show new insights
5. **OFFER SAME KEYWORDS**: Use the format from `<KeywordPresentationTemplate>` to present keywords based on updated verdict (no more "investigate further" to prevent loops)

### Parameter Definitions

When using this workflow template, each review command must provide:

- **[ITEM]**: Singular name for review item (e.g., "recommendation", "issue", "discrepancy")
- **[REVIEW_SPECIFIC_ACTION]**: The unique action for this keyword/review type combination
- **[CUMULATIVE_REVIEW_SPECIFICS]**: How to account for accumulated changes (e.g., "adjust line numbers", "remove resolved issues", "update plan references")

### Review-Specific Action Examples

**Design Review Actions**:
- **"agree"**: UPDATE PLAN DOCUMENT ONLY - Add new dedicated section with full implementation details
- **"skip"**: UPDATE SKIP NOTES - Add to "Design Review Skip Notes" section
- **"skip with prejudice"**: UPDATE SKIP NOTES WITH PREJUDICE FLAG - Mark as permanently rejected

**Code Review Actions**:
- **"fix"**: IMPLEMENT IMMEDIATELY - Apply suggested code change + RUN BUILD AND FORMAT
- **"skip"**: Add to skipped issues list (in memory for final summary)
- **"accept"**: Accept recommendation to not fix
- **"override"**: Fix anyway despite recommendation

**Plan Alignment Actions**:
- **"align to plan"**: IMPLEMENT ALIGNMENT - Modify code to match plan specification
- **"skip"**: Add to skipped items list (in memory for final summary)
- **"accept as built"**: UPDATE PLAN DOCUMENT - Add deviation note to the plan

### Cumulative Review Specifics Examples

**Design Review**:
"Update the suggestion to account for the cumulative effect of these changes - leverage newly added patterns, avoid redundant suggestions, and ensure consistency with what has already been approved."

**Code Review**:
"Update remaining issues to account for the cumulative changes - adjust line numbers, remove issues already resolved by previous fixes, and modify suggestions to align with the current state of the code."

**Plan Alignment**:
"Update remaining discrepancies to account for the cumulative changes - some mismatches may now be resolved, line numbers may have shifted, and new considerations may apply."
</KeywordResponseWorkflow>

<KeywordPresentationTemplate>
## MANDATORY Keyword Presentation Format
**CRITICAL**: ALL review types MUST use this EXACT format when presenting keywords to users.

**Required Format**:
```
Available keywords for [VERDICT] verdict:
- "[keyword]" - ACTION: [Specific action that will be taken]
- "[keyword]" - ACTION: [Specific action that will be taken]
- "[keyword]" - ACTION: [Specific action that will be taken]
- "[keyword]" - ACTION: [Specific action that will be taken if applicable]
```

**Template Variables**:
- [VERDICT]: The investigation verdict (CONFIRMED, FIX RECOMMENDED, ALIGN RECOMMENDED, etc.)
- [keyword]: The exact keyword the user must type
- [Specific action that will be taken]: Clear description of what happens when user chooses this keyword

**ENFORCEMENT**: Any keyword presentation not using this format is INCORRECT and must be corrected.
</KeywordPresentationTemplate>

<CorePresentationFlow>
## Initial Review Summary (MANDATORY)
After receiving the subagent's review and filtering:

1. **Present Summary Statistics**:
   ```
   [Review Type] Summary:
   - Total [items] found: [X]
   - Already addressed (filtered): [Y]
   - [Items] to review: [Z]
   ```
   Note: Replace [Review Type] with "Design Review", "Code Review", or "Plan Alignment Review"
   Replace [items] with "recommendations", "issues", or "discrepancies" as appropriate

2. **Acknowledge Positive Findings** (if applicable):
   List positive findings as bullet points to acknowledge good practices/alignment
   Use section title appropriate to review type:
   - "Positive Findings (Good Practices Observed)" for code review
   - "Well-Aligned Areas (Following Plan Correctly)" for alignment review
   - Skip this step if no positive findings to report

3. **Present Overview Summary**:
   ```
   ‚ïê‚ïê‚ïê‚ïê Items Overview ‚ïê‚ïê‚ïê‚ïê

   High Priority ([count] items):
     ‚Ä¢ [TYPE]-[#]: [Category] - [Brief description]
     ‚Ä¢ [TYPE]-[#]: [Category] - [Brief description]

   Medium Priority ([count] items):
     ‚Ä¢ [TYPE]-[#]: [Category] - [Brief description]
     ‚Ä¢ [TYPE]-[#]: [Category] - [Brief description]

   Low Priority ([count] items):
     ‚Ä¢ [TYPE]-[#]: [Category] - [Brief description]

   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
   ```
   Categories vary by review type but follow pattern: TYPE-SYSTEM, QUALITY, COMPLEXITY, etc.

4. **Transition Statement**:
   "Let's review each [item]. I'll present them one at a time for your decision."
   Replace [item] with "recommendation", "issue", or "discrepancy" as appropriate
</CorePresentationFlow>

<KeywordDecisionProcess>
## Keyword Decision Process

1. **FILTER STEP**: Review each item and exclude any that are already addressed in the review scope
   - For design review: Check Skip Notes section for previously skipped items
   - For code review: Check if issue was already fixed by previous changes
   - For alignment review: Check if discrepancy was resolved by prior alignments

2. **Create todo list** using TodoWrite with ONLY the filtered items as separate todo items
   Each todo should be the item ID and brief description

3. **Present the FIRST item** with full details from the subagent's structured output

4. **MANDATORY: PRESENT KEYWORDS**: Use the exact format from `<KeywordPresentationTemplate>` section
   Then STOP

5. **CRITICAL**: Wait for user to respond with EXACTLY one of the specified keywords

### Auto-Investigation Mode
When using parallel investigation pattern:
- Follow <ParallelInvestigationPattern> after initial review
- All findings come pre-investigated with verdicts
- Post-investigation keywords are presented based on verdict
- "investigate further" is available for targeted follow-up analysis when user needs deeper exploration
</KeywordDecisionProcess>

<KeywordResponseFlow>
## Keyword Response Flow

For each keyword response:

1. **Execute the specified action** for that keyword
2. **Mark current todo as completed**
3. **CRITICAL UPDATE STEP**: Before presenting the next item, review ALL changes made in this session so far:
   - For code changes: Update line numbers, remove resolved issues, adjust suggestions for current state
   - For plan updates: Account for newly added sections, avoid redundant suggestions
   - For alignment fixes: Check if other discrepancies are now resolved
4. **Present next item** with full details
5. **MANDATORY: PRESENT KEYWORDS**: Use the exact format from `<KeywordPresentationTemplate>` section
6. **STOP and wait** for user response

Continue this cycle until all todos are completed.
</KeywordResponseFlow>

<InvestigationPattern>
## Investigation Pattern

When user responds with investigation keyword:

1. **TASK INVESTIGATION AGENT**: Use Task tool to deep-dive investigate with a general-purpose agent acting as a **responsible judge balancing [DOMAIN VALUES]**:

   **CRITICAL JUDGMENT MANDATE**: Act as a responsible engineering judge who values both [PRIMARY VALUE] AND [BALANCING VALUE]

   Standard investigation criteria (customize per review type):
   - **Balance aesthetics vs pragmatism**: Weigh the beauty of clean abstractions against real-world implementation costs
   - **Provide sound engineering judgment**: Consider maintenance burden, team cognitive load, and actual business value
   - **Analyze whether the recommendation provides genuine value vs over-engineering**: Be skeptical of complexity for complexity's sake
   - **Research alternative approaches and trade-offs**: Find the sweet spot between perfectionism and pragmatism
   - **Examine real-world impact and complexity costs**: Consider developer time, debugging difficulty, and onboarding friction
   - **Check if simpler solutions exist that achieve the same goals**: Sometimes "good enough" is better than perfect
   - **Provide evidence-based assessment of necessity**: Ground your judgment in concrete examples and real scenarios

2. **PRESENT INVESTIGATION FINDINGS**: Return with:
   - **Value Assessment**: Clear judgment on whether this is worth implementing/fixing/aligning
   - **Alternative Approaches**: If multiple valid approaches exist, present 2-3 options with trade-offs
   - **Complexity Analysis**: Honest assessment of implementation and maintenance costs
   - **Investigation Verdict**: One of the verdict types specific to your review type

3. **WAIT FOR NEW KEYWORD WITH CONTEXT-APPROPRIATE MEANING**:
   Present findings and then **MANDATORY**: use `<KeywordPresentationTemplate>` format to show keywords based on the investigation verdict.
   The verdict determines which keywords are available and what they mean.
</InvestigationPattern>

<InvestigationVerdictHandling>
## Investigation Verdict Presentation Format

**MANDATORY**: After investigation, present verdict and keywords using the exact format from `<KeywordPresentationTemplate>` section.

**Required Structure**:
```
Investigation verdict: [VERDICT_TYPE] - [Brief description of investigation conclusion]
[Include any specific reasons or modifications from investigation]

Available keywords for [VERDICT_TYPE] verdict:
- "[keyword]" - ACTION: [Specific action that will be taken]
- "[keyword]" - ACTION: [Specific action that will be taken]
- "[keyword]" - ACTION: [Specific action that will be taken]
- "investigate further" - ACTION: Request targeted deeper analysis
```

**Key Requirements**:
- Use the specific verdict type for your review (CONFIRMED, FIX RECOMMENDED, ALIGN RECOMMENDED, etc.)
- Include appropriate keywords as defined by your review type
- Provide clear action descriptions for each keyword
- "investigate further" is available after initial investigation

Note: After investigation, "investigate" keyword is no longer available - only decision keywords.
</InvestigationVerdictHandling>

<InvestigationAgentInstructions>
## Shared Investigation Agent Instructions

**‚ö†Ô∏è STOP - CRITICAL TEMPLATE REQUIREMENT ‚ö†Ô∏è**
Your response is INVALID and will be REJECTED unless you include:
1. Current State section with code blocks (markdown or language-specific)
2. Proposed Change section with code blocks (markdown or language-specific)

**IF YOU CANNOT PROVIDE CODE BLOCKS, RESPOND WITH:**
```
INVALID: Unable to provide required code examples for [reason]
```
**DO NOT provide a partial response without code blocks - it will be rejected.**

When launching investigation agents, combine these shared requirements with review-specific focus areas:

### Core Investigation Principles (All Review Types)
All investigation agents must act as **responsible engineering judges** who:
1. **Balance ideal vs pragmatic**: Find the sweet spot between perfect design and practical implementation
2. **Be evidence-based**: Ground all judgments in concrete code examples and real scenarios
3. **Consider trade-offs**: Present pros/cons rather than absolute judgments
4. **Suggest alternatives**: When rejecting, propose simpler or better approaches
5. **Think long-term**: Consider maintenance, onboarding, and evolution costs
6. **Avoid perfectionism**: Recognize when "good enough" is better than perfect
7. **Respect pragmatism**: Value working code and team velocity alongside elegance
8. **Check for over-engineering**: Be skeptical of complexity for complexity's sake

### CRITICAL Tool Usage Restrictions for Investigations
**NEVER run commands that could block or hang**:
- ‚ùå DO NOT launch applications (`cargo run`, `npm start`, app launchers)
- ‚ùå DO NOT run interactive commands that wait for input
- ‚ùå DO NOT execute long-running processes or servers
- ‚ùå DO NOT run watch commands or continuous monitors

**ONLY use these safe tools**:
- ‚úÖ Read tool - examine files
- ‚úÖ Grep/Glob tools - search codebase
- ‚úÖ WebSearch/WebFetch - research information
- ‚úÖ Quick analysis commands that return immediately (`cargo check --message-format json`, `git log -1`)

**If you need runtime information**: Read existing logs, test outputs, or documentation instead of running the application

### Investigation Process
Each investigation should:
1. Analyze the specific finding/issue/discrepancy in context
2. **PRESERVE the current and proposed code examples from the original finding**
3. Apply the review-specific investigation focus (provided by each review command)
4. Consider both immediate and long-term implications
5. Evaluate alternative approaches if relevant
6. If suggesting modifications, provide **MODIFIED CODE** showing the improved version
7. Provide a clear verdict with rationale

### MANDATORY Investigation Output Template - CRITICAL COMPLIANCE

**TEMPLATE ENFORCEMENT**: Investigation agents MUST copy and fill out this EXACT template. Any response not using this structure is INVALID and will be rejected.

**START TEMPLATE - COPY EXACTLY:**

```
Investigation Verdict: [VERDICT_TYPE - e.g., CONFIRMED, MODIFIED, REJECTED]

Rationale: [2-3 sentence explanation of verdict]

## Current State
[Choose appropriate format based on review type]

### For Design Review - Current Plan:
```markdown
[Quote what the plan currently says about this area]
```

### For Design/Code Review - Current Code:
```[language]
// File: path/to/file.rs (lines X-Y)
[Exact code from files showing the current state]
```

## Proposed Change
[Choose appropriate format based on review type]

### For Design Review - Proposed Plan Addition:
```markdown
[Exact text to add to the plan document]
```

### For All Reviews - Proposed Code:
```[language]
// This code demonstrates the proposed solution:
[Concrete code example showing the improvement]
```

Trade-offs:
- Pro: [Main benefit]
- Con: [Main drawback if any]

Complexity: [Low/Medium/High] - [Brief explanation]

[If MODIFIED verdict, include:]
Alternative Approach:
[Describe the modification to the original recommendation]
```
**END TEMPLATE**

**MANDATORY COMPLIANCE CHECKLIST** - Investigation agents must verify:
- [ ] Used exact template structure above
- [ ] Included Current State section with code blocks
- [ ] Included Proposed Change section with code blocks
- [ ] All code blocks show concrete examples, not descriptions
- [ ] Template sections are complete, not summarized

**ENFORCEMENT**: Any investigation missing these elements will be immediately rejected and must be redone.

### ‚úÖ VALID Investigation Example:
```
Investigation Verdict: CONFIRMED
Rationale: The plan is missing critical implementation examples for Map types.

## Current State
### For Design Review - Current Plan:
```markdown
The plan currently only shows examples for List types but omits Map entirely.
```

## Proposed Change
### For Design Review - Proposed Plan Addition:
```markdown
Add this section to Step 3: Map types should validate value types inline...
```

Trade-offs:
- Pro: Completes the pattern
- Con: Adds complexity
Complexity: Low
```

### ‚ùå INVALID Investigation Example (WILL BE REJECTED):
```
Investigation Verdict: CONFIRMED
Rationale: This needs improvement.

The current implementation has issues that should be fixed by improving the design.
I recommend updating the plan to address these concerns.

Trade-offs: Various considerations apply.
```
**WHY INVALID**: Missing Current State section, missing Proposed Change section, no code blocks!

### Placeholder Values for Review Commands
Each review command should provide:
- **[DOMAIN VALUES]**: The specific balance to maintain (e.g., "code elegance AND practical utility")
- **[INVESTIGATION FOCUS]**: Specific criteria for that review type
- **[EXPECTED VERDICTS]**: The verdict types for that review
</InvestigationAgentInstructions>

<ParallelInvestigationPattern>
## Parallel Investigation Pattern (Auto-Investigation Mode)

When configured for auto-investigation, the review process changes:

1. **INITIAL SUBAGENT REVIEW**: Get findings from initial review agent as normal

2. **FILTER RESULTS**: Remove any items already addressed:
   - For design review: Check Skip Notes section for previously skipped items
   - For code review: Check if issue was already fixed by previous changes
   - For alignment review: Check if discrepancy was resolved by prior alignments

3. **PRESENT FILTERED RESULTS SUMMARY**: Display filtered results using this template:

   **[Review Type] Results Summary:**
   ```
   Total [items] found: [X]
   Already addressed (filtered out): [Y]
   Remaining [items] to investigate: [Z]

   [Items] by category:
   - [CATEGORY-1]: [count] [items] ([priority breakdown if applicable])
   - [CATEGORY-2]: [count] [items] ([priority breakdown if applicable])
   - [etc.]

   [Items] by priority:
   - High: [count] [items]
   - Medium: [count] [items]
   - Low: [count] [items]
   ```

   Replace placeholders:
   - [Review Type]: "Design Review", "Code Review", or "Plan Alignment Review"
   - [items]: "recommendations", "issues", or "discrepancies" as appropriate
   - [CATEGORY-X]: TYPE-SYSTEM, QUALITY, COMPLEXITY, MISSING, MISMATCH, etc.

4. **AUTOMATIC PARALLEL INVESTIGATION**:
   - Launch parallel investigation agents for ALL filtered findings simultaneously
   - Create parallel Task tool invocations in a single response
   - Each agent MUST be instructed to:
     - Follow the Required Investigation Output Template from <InvestigationAgentInstructions>
     - Include Current State and Proposed Change sections WITH CODE BLOCKS
     - Return INVALID if unable to provide code examples
   - Each agent receives:
     - The specific finding/issue/discrepancy with its code examples
     - Standard investigation criteria from <InvestigationPattern>
     - Domain-specific balance values (elegance vs utility, quality vs pragmatism, etc.)
     - EXPLICIT instruction: "You MUST use the Required Investigation Output Template with code blocks"
   - Wait for all investigations to complete

4. **COLLECT AND MERGE**:
   - **VALIDATION STEP**: Check each investigation result for required template compliance
   - **REJECT** investigations missing Current State or Proposed Change sections with code blocks
   - **REJECTION HANDLING**:
     - When an investigation is rejected for non-compliance:
       1. DO NOT include it in the todo list
       2. DO NOT present it to the user
       3. Log: "Investigation for [FINDING-ID] failed validation - missing required code blocks"
       4. EITHER: Re-launch investigation with stricter enforcement: "Previous response was INVALID. You MUST include code blocks."
       5. OR: Skip the finding entirely with note in final summary: "Unable to investigate: [FINDING-ID]"
   - For compliant investigations: Combine original finding with investigation results
   - Include verdict, value assessment, alternatives, complexity analysis
   - Prepare enriched findings for presentation

5. **CREATE TODO LIST**: Build todos with enriched findings

5.5. **PRE-PRESENTATION VALIDATION GATE**:
   Before presenting ANY finding to the user:
   - **CHECK**: Does investigation result have "## Current State" with code blocks?
     - If NO ‚Üí SKIP this finding, log "Skipped [FINDING-ID]: missing Current State"
   - **CHECK**: Does investigation result have "## Proposed Change" with code blocks?
     - If NO ‚Üí SKIP this finding, log "Skipped [FINDING-ID]: missing Proposed Change"
   - **ONLY** findings passing BOTH checks proceed to presentation
   - Keep count of skipped findings for final summary

6. **PRESENT PRE-INVESTIGATED FINDINGS ONE AT A TIME**:
   **CRITICAL**: Present ONLY THE FIRST finding, then STOP and wait for user response.
   NEVER present multiple findings at once, even if they seem related.
   Follow <EnforcementRules> section - specifically rule #4 "NO BATCHING".

   Present the first finding using this structure:

   ```
   [CATEGORY-ID]: [Title]

   Investigation Verdict: [VERDICT] ‚úÖ/‚ö†Ô∏è/‚ùå

   [Rationale from investigation]

   ## Current State
   [Current Plan/Code from investigation - MUST include code blocks]

   ## Proposed Change
   [Proposed Plan/Code from investigation - MUST include code blocks]

   Trade-offs:
   [From investigation]

   Complexity: [From investigation]

   [Use format from `<KeywordPresentationTemplate>` section]
   ```

   **CRITICAL**: If investigation results don't include Current State and Proposed Change sections with code blocks, DO NOT present the finding - request investigation correction first

   **MANDATORY**: After presenting this ONE finding with its keywords, STOP.
   Do not present the next finding until the user responds with a keyword.
   This is non-negotiable - see <EnforcementRules> for strict enforcement.
</ParallelInvestigationPattern>

<InvestigateFurtherPattern>
## Investigate Further Pattern

When user responds "investigate further" (available after initial investigation):

1. **PROMPT FOR GUIDANCE**: Ask user:
   "What specific aspect would you like me to investigate further?"

2. **TASK FOCUSED INVESTIGATION**: Launch new investigation with:
   - Original finding context
   - Previous investigation results
   - User's specific guidance
   - Targeted analysis based on user direction

3. **PRESENT SUPPLEMENTAL FINDINGS**:
   - Show new insights from focused investigation
   - Update verdict if findings change recommendation
   - **MANDATORY**: Present same keyword options using `<KeywordPresentationTemplate>` format (excluding "investigate further" to prevent loops)

4. **KEYWORD OPTIONS AFTER FURTHER INVESTIGATION**:
   - Same as original post-investigation keywords
   - No additional "investigate further" to prevent infinite loops
   - User must make decision after one round of further investigation
</InvestigateFurtherPattern>

<CumulativeUpdateRule>
## CRITICAL Cumulative Update Rule

**This rule is MANDATORY and must be applied after EVERY keyword response before presenting the next item:**

Before presenting the next item, you MUST:

1. **Review ALL changes** made in this session so far:
   - What has been implemented/modified/accepted/skipped
   - How these changes affect remaining items

2. **Update remaining items** to account for cumulative effects:
   - **Line number adjustments**: If code was modified, line numbers in remaining items may have shifted
   - **Resolution detection**: Some items may have been indirectly resolved by previous changes
   - **Dependency updates**: If an item depended on something now changed, update its description
   - **Context changes**: Suggestions may need modification based on newly accepted patterns

3. **Filter out resolved items**: Don't present items that were fixed as a side effect of other changes

4. **Modify suggestions if needed**: Update recommendations to align with decisions already made

Example scenarios:
- If you accepted a new enum pattern, update other recommendations to use that pattern
- If you fixed an error handling approach, apply it consistently in remaining suggestions
- If you aligned one API, check if others need the same alignment

**NEVER present an item without considering how previous session changes affect it.**
</CumulativeUpdateRule>

<EnforcementRules>
## Keyword Enforcement Rules

These rules are MANDATORY and apply to all review types:

1. **NO OTHER RESPONSES ACCEPTED**:
   - Only the specified keywords trigger actions
   - If user types anything else, politely restate the keyword options using `<KeywordPresentationTemplate>` format
   - Do not interpret or guess user intent from non-keyword responses

2. **MANDATORY STOPPING**:
   - ALWAYS stop after each item presentation
   - Never continue to the next item without user keyword response
   - Even if only one item remains, still stop and wait

3. **NO ASSUMPTIONS**:
   - Never assume user intent - wait for explicit keyword
   - Don't skip items assuming user doesn't care
   - Don't batch similar items together

4. **NO BATCHING**:
   - Process exactly one item per user keyword response
   - Never group items even if they seem related
   - Each item gets its own presentation and decision

5. **INVESTIGATE LIMITATION**:
   - "investigate" keyword only available on first presentation of an item
   - After investigation completes, only decision keywords are accepted
   - Cannot investigate the same item twice

6. **KEYWORD CONSISTENCY**:
   - Use the exact keywords defined for your review type
   - Don't create variations or shortcuts
   - Keywords must match exactly (case-insensitive is acceptable)
</EnforcementRules>

<FinalSummaryTemplate>
## Final Summary Template

When all items are addressed, provide a summary using this structure:

```
[Review Type] Complete!

**Results Summary:**
- [Primary Action Taken]: [X]
  - [Subcategory 1]: [count]
  - [Subcategory 2]: [count]

- [Secondary Action Taken]: [Y]
  - [List items with brief descriptions]
  - [Note where documented if applicable]

- [Positive Aspect Maintained]: [Z]
  - [Quick recap of what was good]

[Closing statement appropriate to review type]
```

Examples:
- Design Review: "Plan document has been updated with approved recommendations."
- Code Review: "All changes have been applied to your working directory."
- Alignment Review: "Plan and implementation are now synchronized."
</FinalSummaryTemplate>

<SpecialCaseHandling>
## Special Case Handling

Some review types have unique keywords or special handling:

### "skip with prejudice" (Design Review only)
- Used when reviewer repeatedly suggests already-rejected items
- Creates permanent rejection with ‚ö†Ô∏è PREJUDICE WARNING
- Strongest form of rejection
- Include warning: "DO NOT SUGGEST THIS AGAIN"

### "override" keyword (After negative investigation verdict)
- Only available when investigation recommends NOT doing something
- Allows user to proceed despite negative recommendation
- Document both investigation findings and override rationale

### "accept as built" (Alignment Review only)
- Unique to alignment review where deviation can be accepted
- Updates plan to match implementation rather than vice versa
- Documents the accepted deviation with rationale

### Multiple verdict types
Some reviews have more complex verdict structures:
- Design Review: CONFIRMED, MODIFIED, REJECTED
- Code Review: FIX RECOMMENDED, FIX MODIFIED, FIX NOT RECOMMENDED
- Alignment Review: ALIGN RECOMMENDED, ACCEPT RECOMMENDED, DEFER RECOMMENDED

Each verdict type should have appropriate keyword options that make semantic sense, presented using `<KeywordPresentationTemplate>` format.
</SpecialCaseHandling>

<TypeSystemDesignPrinciples>
## Type System Design Principles (Shared)

These principles form the foundation for design and code reviews focused on type-driven development:

### Primary Type System Violations (ALWAYS High Priority)

1. **Conditional Audit**: Every if-else chain is a design failure
   - String-based conditionals that could be replaced with enums and pattern matching
   - Boolean flags tracking state that should use state enums
   - Deeply nested conditionals indicating failure to model the domain properly
   - TREAT CONDITIONALS AS THE ENEMY - each one is a missed opportunity

2. **Function vs Method Audit**: Every standalone utility function is suspect
   - Functions that should be methods on a type that owns the behavior
   - Utility functions that should be trait implementations for polymorphic behavior
   - Functions that could be eliminated through better type design

3. **String Typing Violations**: Every string representing finite values should be an enum
   - Stringly-typed APIs that should use proper types
   - String parameters that represent known constants or variants
   - String data that should be wrapped in type-safe structures
   - **EXCEPTIONS**: Format validation (email, URLs), arbitrary text processing, string accessors on typed data

4. **State Machine Failures**: State tracking with primitives instead of types
   - Boolean flags that should be part of state enums
   - Multiple booleans that represent mutually exclusive states
   - String values tracking application state

5. **Builder Pattern Opportunities**: Complex construction that needs structure
   - Functions with many parameters that should use builder pattern
   - Complex initialization logic scattered across multiple calls

### Error Handling Standards

1. **Safe vs Unsafe Patterns**:
   - **ACCEPTABLE**: `unwrap_or()` and `unwrap_or_else()` with fallbacks
   - **FLAG FOR FIXING**: Bare `.unwrap()` calls without fallbacks
   - **PREFERRED**: Proper Result/Option usage with explicit error handling

2. **Error Context Requirements**:
   - Missing error contexts that make debugging difficult
   - Generic error messages that don't help identify root causes

### Design Quality Standards

1. **Complexity Reduction**:
   - Unnecessary nesting that can be simplified
   - Long functions that should be broken down
   - Unclear logic that needs refactoring

2. **Code Duplication**:
   - Copy-pasted patterns that should be abstracted
   - Similar logic that should share common implementations

3. **API Design**:
   - Inconsistent naming conventions
   - Missing documentation for complex public APIs
   - Type signatures that don't express intent clearly

### Analysis Priority Order

1. **FIRST**: Type system violations (highest priority, always flag)
2. **SECOND**: Error handling and safety concerns
3. **THIRD**: Code complexity and clarity issues
4. **FOURTH**: Duplication and maintenance concerns
5. **FIFTH**: Documentation and API design

### What NOT to Flag

1. **Acceptable String Usage**:
   - String accessor methods on well-typed data (e.g., `enum.name()`)
   - String data already contained in type-safe structures
   - Format validation patterns (email, URL parsing, etc.)
   - Arbitrary text processing where enums don't make sense

2. **Acceptable Error Patterns**:
   - `unwrap_or()` and `unwrap_or_else()` with proper fallbacks
   - Standard boilerplate error handling in tests

3. **Standard Patterns**:
   - Conventional project structure and imports
   - Standard library usage following established patterns
</TypeSystemDesignPrinciples>

<CustomizationInstructions>
## How to Use This Pattern File

When implementing a review command, reference this file and specify:

1. **Which tagged sections to follow** completely:
   - Example: "Follow <CorePresentationFlow> exactly as specified"
   - Example: "Follow <TypeSystemDesignPrinciples> for analysis requirements"

2. **Your specific values for placeholders**:
   - [Review Type]: "Design Review", "Code Review", "Plan Alignment Review"
   - [items]: "recommendations", "issues", "discrepancies"
   - [DOMAIN VALUES]: "code elegance AND practical utility"

3. **Your specific keywords and their actions**:
   - List exact keywords for your review type
   - Define precise ACTION descriptions

4. **Your investigation verdicts**:
   - Define verdict types specific to your domain
   - Specify which keywords are available for each verdict

5. **Any special cases** unique to your review type:
   - Additional keywords like "skip with prejudice"
   - Special handling rules

6. **What to keep in your file**:
   - Unique keyword response actions
   - Specific document/code modifications
   - Review-specific analysis requirements
   - Custom prompt templates for the subagent

This pattern file provides the structure; your review file provides the specifics.
</CustomizationInstructions>
